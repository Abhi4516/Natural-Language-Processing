{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70e7d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=' NaTuRAL_lANGUAGE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2edf64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a26a03fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NaTuRAL_lANGUAGE'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "386c930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TuR\n",
      "..............................\n"
     ]
    }
   ],
   "source": [
    "print(nlp[3:6])\n",
    "print(\"...\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8cd6dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NaTuRAL_lANGUAGE']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43fd5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' natural_language'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "782eb0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NATURAL_LANGUAGE'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "959605e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NaTuRAL_lANGUAGe'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.replace('E','e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f12dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' NaTuRAL', 'lANGUAGE']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36813ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NaTuRAL_lANGUAGE processing'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n='processing'\n",
    "a=nlp+\" \"+n\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b61cecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are u\n"
     ]
    }
   ],
   "source": [
    "with open('nlp.txt','r') as file:\n",
    "    f=file.read()\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5d37cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are u'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('nlp.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b496e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"iNeuron.txt\", \"a+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "507803ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    file.write(\"Appending Line number %d\\r\\n\" %(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1417a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web srcapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0154e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dffc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://ineuron.ai/'\n",
    "response=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f7f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca4e4f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/courses\n",
      "https://support.ineuron.ai\n",
      "/courses\n",
      "/courses\n",
      "https://halloffame.ineuron.ai/\n",
      "https://halloffame.ineuron.ai/\n",
      "https://neurolab.ineuron.ai\n",
      "https://jobs.ineuron.ai\n",
      "/one-neuron\n",
      "https://support.ineuron.ai\n",
      "https://internship.ineuron.ai\n",
      "https://affiliate.ineuron.ai\n",
      "https://halloffame.ineuron.ai\n",
      "https://hackathon.ineuron.ai\n",
      "https://ineuron.ai\n",
      "mailto:contact@ineuron.ai\n",
      "tel:+918071176111\n",
      "https://www.facebook.com/officialpwskills\n",
      "https://www.instagram.com/official_ineuron.ai\n",
      "https://www.youtube.com/@iNeuroniNtelligence\n",
      " https://twitter.com/ineuron_ai\n",
      "https://www.linkedin.com/company/ineuron-ai\n",
      "https://discord.com/invite/wyEAPaQEBN\n",
      "/about-us\n",
      "/contact-us\n",
      "/faqs\n",
      "/certificate-verification\n",
      "/privacy-policy\n",
      "/terms-and-conditions\n",
      "/one-neuron\n",
      "https://support.ineuron.ai\n",
      "https://neurolab.ineuron.ai\n",
      "https://jobs.ineuron.ai\n",
      "https://internship.ineuron.ai\n",
      "https://affiliate.ineuron.ai\n",
      "https://halloffame.ineuron.ai\n",
      "https://hackathon.ineuron.ai\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "     print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a96fb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy.possiblity of rain is high,today!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text): \n",
    "    return text.lower() \n",
    "  \n",
    "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
    "lowercase_text(input_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc70571b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  \n",
    "def remove_num(text): \n",
    "    result = re.sub(r'\\d', '', text) \n",
    "    return result \n",
    "  \n",
    "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(input_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "001237f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'           6                        4                     '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\D',\" \",input_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e171f825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Youbought6candiesfromshop,and4candiesareinhome.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\s',\"\",input_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02df0c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john@example.com\n",
      "john.doe@example.org\n",
      "jane_doe123@example.co.uk\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(text):\n",
    "    \n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}'\n",
    "    \n",
    "\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "\n",
    "example_text = \"\"\"\n",
    "Hello, my email is john@example.com. Please contact me at john.doe@example.org\n",
    "or at jane_doe123@example.co.uk. Thanks!\n",
    "\"\"\"\n",
    "\n",
    "extracted_emails = extract_emails(example_text)\n",
    "\n",
    "for email in extracted_emails:\n",
    "    print(email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a17929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5f2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=inflect.engine()\n",
    "q.number_to_words(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb78726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t='he main written part 3 of a book, newspaper,  10 etc. not the pictures, notes, index, etc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b30e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l=t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a401908f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w='123'\n",
    "w.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eece54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11dbcf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d2e3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_punct(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "  \n",
    "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "rem_punct(input_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db45d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed47016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88c564c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords(text):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_token=word_tokenize(text)\n",
    "    filtered_text = [word for word in word_token if word  in stop_words] \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1c48c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=\"The sun was setting behind the mountains, casting a warm glow over the valley below. Birds chirped happily as they flew back to their nests, and a gentle breeze rustled the leaves of the trees. In the distance, you could hear the sound of a stream babbling softly, adding to the tranquility of the scene. It was a peaceful evening, perfect for a leisurely stroll in nature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18d17551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was',\n",
       " 'the',\n",
       " 'a',\n",
       " 'over',\n",
       " 'the',\n",
       " 'below',\n",
       " 'as',\n",
       " 'they',\n",
       " 'to',\n",
       " 'their',\n",
       " 'and',\n",
       " 'a',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " 'you',\n",
       " 'the',\n",
       " 'of',\n",
       " 'a',\n",
       " 'to',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'was',\n",
       " 'a',\n",
       " 'for',\n",
       " 'a',\n",
       " 'in']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_stopwords(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60de7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5f7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3a7178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c5a48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "stem1 = PorterStemmer() \n",
    "  \n",
    "\n",
    "def s_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stem1.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b241a661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is', 'abhi']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('hello my name is abhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dccba6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\ASUS\n",
      "[nltk_data]     ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import wordnet \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    " \n",
    "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e8ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('running',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01f083f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ASUS\n",
      "[nltk_data]     ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc70fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33497831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c9449eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "\n",
    "def pos_tagg(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens) \n",
    "  \n",
    "pos_tagg('Are you afraid of something?') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d59688a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\Users\\ASUS\n",
      "[nltk_data]     ZenBook\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets') \n",
    " \n",
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90d8f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "  \n",
    " \n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "\n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "\n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "  \n",
    "\n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "\n",
    "      \n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d488b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\ASUS\n",
      "[nltk_data]     ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\ASUS\n",
      "[nltk_data]     ZenBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag, ne_chunk \n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d95be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  (GPE Test/NNP)\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  england/VB\n",
      "  am/VBP)\n"
     ]
    }
   ],
   "source": [
    "def ner(text): \n",
    " \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "\n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    " \n",
    "    print(ne_chunk(word_pos)) \n",
    "  \n",
    "text = 'Brain Lara scored the highest 400 runs in a Test match which played in between WI and england am'\n",
    "ner(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c0a38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iNeuron13']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = \"iNeuron13 ,data is a new fuel\"\n",
    "r2 = re.findall(r\"^\\w+\",sent)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba938aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'splited', 'this', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print((re.split(r'\\s','We splited this sentence')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1c50810",
   "metadata": {},
   "outputs": [],
   "source": [
    "cout=word_tokenize('There is no need to panic. We need to work together, take small yet important measures to ensure self-protection, the Prime Minister tweeted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57457a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20a3eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "m = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abb4d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "tokens = WhitespaceTokenizer().tokenize(m)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c862d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de0995e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<FreqDist with 23 samples and 28 outcomes>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "text1 = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "freqDist = nltk.FreqDist(word_tokenize(text1))\n",
    "print(freqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77ed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0680c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ASUSZE~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.654 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'am', ' ', 'going', ' ', 'to', ' ', 'the', ' ', 'United', ' ', 'States']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    " \n",
    "text = \"I am going to the United States\"\n",
    "cut = jieba.cut(text)\n",
    "sent = list(cut)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c508d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will', 'will go', 'go to', 'to United', 'United States']\n"
     ]
    }
   ],
   "source": [
    "Sent = \"I will go to United States\"\n",
    "lst_sent = Sent.split (\" \")\n",
    "of_bigrams_in = []\n",
    "for i in range(len(lst_sent)- 1):\n",
    "   of_bigrams_in.append(lst_sent[i]+ \" \" + lst_sent[ i + 1])\n",
    "   \n",
    "    \n",
    "print(of_bigrams_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3b4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation_pattern = re.compile(r\"\" \"[.,!? \"\"] \"\" \" )\n",
    "\n",
    "sent = \"I will go to United States\"\n",
    "no_punctuation_sent = re.sub(punctuation_pattern , \" \" , sent )\n",
    "lst_sent = no_punctuation_sent.split (\" \")\n",
    "trigram = []\n",
    "for i in range(len(lst_sent)- 2):\n",
    "   trigram.append(lst_sent[i] + \" \" + lst_sent[i + 1] + \" \" +lst_sent[i + 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af71f2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I will go', 'will go to', 'go to United', 'to United States']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cc6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
